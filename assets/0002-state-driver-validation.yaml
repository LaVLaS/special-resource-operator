apiVersion: v1
kind: ConfigMap
metadata:
  name: entrypoint
data:
  entrypoint.sh: |-
    #!/bin/bash
    NUM_GPUS=$(nvidia-smi -L | wc -l)
    for i in `seq 0 $(($NUM_GPUS-1))`
    do 
      echo "Executing VectorAdd on GPU$i"
      CUDA_VISIBLE_DEVICES=$i /usr/local/cuda/samples/0_Simple/vectorAdd/vectorAdd
      if [ ! $? -eq 0 ];then 
        exit 1
      fi
    done

    if [ $NUM_GPUS -eq 0 ]; then
      echo "ERROR No GPUs found"
      exit 1
    fi
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nvidia-driver-validation
  name: nvidia-driver-validation
  namespace: openshift-sro
  annotations:
    callback: nvidia-driver-validation
spec:
#  serviceAccount: nvidia-readonly
#  serviceAccountName: nvidia-readonly
  tolerations:
    - operator: Exists
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  restartPolicy: Never
  containers:
    - name: cuda-vector-add
      image: "quay.io/openshift-psap/cuda-vector-add:v0.1"
      command: ["/bin/entrypoint.sh"]
      volumeMounts:
      - name: entrypoint
        mountPath: /bin/entrypoint.sh
        readOnly: true
        subPath: entrypoint.sh
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: all
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: NVIDIA_REQUIRE_CUDA  
          value: "cuda>=5.0"
      securityContext:
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        capabilities:
          drop: ["ALL"]
  volumes:
  - name: entrypoint
    configMap:
      defaultMode: 0700
      name: entrypoint
